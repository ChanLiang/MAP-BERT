# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import math
import numpy as np

import torch
import torch.nn.functional as F

from fairseq import metrics, utils
from fairseq.criterions import FairseqCriterion, register_criterion


@register_criterion('masked_lm_exp')
class MaskedLmLossExp(FairseqCriterion):
    """
    Implementation for the loss used in masked language model (MLM) training.
    """

    def __init__(self, args, task):
        super(MaskedLmLossExp, self).__init__(args, task)

        self.vocab = self.task.source_dictionary
        self.mask_idx = self.task.mask_idx
        self.mask_prob = self.task.args.mask_prob
        self.leave_unmasked_prob = self.task.args.leave_unmasked_prob
        self.random_token_prob = self.task.args.random_token_prob
        self.rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob

        self.mask_whole_words = self.task.args.mask_whole_words
        self.freq_weighted_replacement = self.task.args.freq_weighted_replacement

        self.do_deterministic = args.masker_deterministic

        self.exp_name = args.exp_name
        if self.random_token_prob > 0.0:
            if self.freq_weighted_replacement:
                weights = np.array(self.vocab.count)
            else:
                weights = np.ones(len(self.vocab))
            weights[:self.vocab.nspecial] = 0
            self.weights = weights / weights.sum()

        self.register_buffer('random_weights', torch.tensor(self.weights).type(torch.float32))

    @staticmethod
    def add_args(parser):
        """Add criterion-specific arguments to the parser."""
        super(MaskedLmLossExp,
              MaskedLmLossExp).add_args(parser)

        parser.add_argument('--exp-name', default='checkpoint0', type=str,
                            help='name for the saved txt file')

        parser.add_argument('--masker_deterministic', default=False,
                            action='store_true',
                            help='is the mask generated by sampling?')

    def forward(self, model, sample, reduce=True):
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        # compute MLM loss
        # model.learner model.lm
        raw_inps = sample["net_input"]["src_tokens"]
        raw_targets = sample['target']
        raw_masked_tokens = raw_targets.ne(self.padding_idx)
        inps = raw_targets * raw_masked_tokens + \
               raw_inps * (raw_masked_tokens ^ True)
        sz = inps.size(-1) # all batches should be the same length
        num_mask = int(sz * 0.15)

        #masker_out = model.masker(inps)[0]#.view(inps.size(0), -1)
        masker_out = torch.ones((1, sz)) / (sz*1.0)
        masker_out[0][0] = 0.0
        masker_out = masker_out.to(inps.device)
        masker_out = masker_out.type(torch.float)
        #print(masker_out.type(), masker_out[0])

        if num_mask == 0:
            num_mask = 1


        LossVecs = []
        GVecs = []


        model.zero_grad()
        for i in range(100):

            print(i)
            with torch.no_grad():
                masked_idxes = torch.multinomial(masker_out, num_mask, replacement=False)


            labels_list = []
            with torch.no_grad():

                #labels = torch.full_like(inps, self.padding_idx)
                #labels[masked_idxes] = inps[masked_idxes]


                rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob

                new_inps = []

                #import IPython
                #IPython.embed()

                for i in range(inps.size(0)):
                    inp = inps[i]
                    mask = torch.full_like(inp, False).type(torch.bool)
                    mask[masked_idxes[i]] = True

                    label = torch.full_like(inp, self.padding_idx)
                    label[masked_idxes[i]] = inp[masked_idxes[i]]
                    labels_list.append(label)

                    #import IPython
                    #IPython.embed()

                    if rand_or_unmask_prob > 0.0:
                        tmp_rand = torch.rand_like(inp.type(torch.float))
                        tmp_rand = (tmp_rand < rand_or_unmask_prob)
                        #tmp_rand = tmp_rand.to(inp.device)
                        #tmp_rand = (torch.rand(sz) < rand_or_unmask_prob).to(mask.device)
                        tmp_rand = tmp_rand.type(mask.type())
                        rand_or_unmask = mask & tmp_rand
                        if self.random_token_prob == 0.0:
                            unmask = rand_or_unmask
                            rand_mask = None
                        elif self.leave_unmasked_prob == 0.0:
                            unmask = None
                            rand_mask = rand_or_unmask
                        else:
                            unmask_prob = self.leave_unmasked_prob / rand_or_unmask_prob
                            decision = torch.rand_like(inp.type(torch.float))  < unmask_prob
                            decision = decision.type(mask.type())
                            unmask = rand_or_unmask & decision
                            rand_mask = rand_or_unmask & (~decision)
                    else:
                        unmask = rand_mask = None

                    if unmask is not None:
                        mask = mask ^ unmask

                    #if self.mask_whole_words is not None:
                    #    #mask = torch.repeat(mask, word_lens)
                    #    mask = mask.repeat(word_lens)



                    new_item = inp.clone()
                    #print('mask, new item', mask.shape, new_item.shape, mask.type(), torch.sum(mask).item())
                    mask_idxs = torch.full_like(new_item, self.mask_idx)
                    new_item = torch.where(mask, mask_idxs, new_item)
                    #new_item[mask] = self.mask_idx
                    if rand_mask is not None:
                        num_rand_int = rand_mask.sum().item()
                        num_rand = rand_mask.sum()

                        #print('num_rand', num_rand_int)
                        if num_rand_int > 0:
                            #if self.mask_whole_words is not None:
                            #    #rand_mask = torch.repeat(rand_mask, word_lens)
                            #    rand_mask = rand_mask.repeat(word_lens)
                            #    num_rand = rand_mask.sum()
                            #import IPython
                            #IPython.embed()
                            # rand_tensor = torch.tensor(
                            #     np.random.choice(len(self.vocab),
                            #                      num_rand.cpu().numpy(),
                            #                      p=self.weights)).to(mask.device)
                            rand_tensor = torch.multinomial(self.random_weights, num_rand,  False)
                            rand_tensor.type(inps.type())
                            new_item[rand_mask] = rand_tensor
                    new_inps.append(new_item)

                new_inp = torch.stack(new_inps, dim=0)
                labels = torch.stack(labels_list, dim=0)

                sample['target'] = labels
                sample['net_input']["src_tokens"] = new_inp
            masked_tokens = sample['target'].ne(self.padding_idx)
            sample_size = masked_tokens.int().sum().item()

            # (Rare case) When all tokens are masked, the model results in empty
            # tensor and gives CUDA error.
            if sample_size == 0:
                masked_tokens = None

            logits = model(**sample['net_input'], masked_tokens=masked_tokens)[0]
            targets = model.get_targets(sample, [logits])

            if sample_size != 0:
                targets = targets[masked_tokens]

            loss = F.nll_loss(
                F.log_softmax(
                    logits.view(-1, logits.size(-1)),
                    dim=-1,
                    dtype=torch.float32,
                ),
                targets.view(-1),
                reduction='sum',
                ignore_index=self.padding_idx,
            )

            #rand_idx = np.random.randint(low=0, high=loss.size(-1), size=1)[0]

            #print(loss.size())
            #token_loss = loss[rand_idx]



            #token_loss.backward()
            loss.backward()

            with torch.no_grad():
                grads = []

                for param in model.parameters():
                    grads.append(param.grad.reshape(-1))

                grad = torch.cat(grads)

                g_norm = torch.norm(grad)

            LossVecs.append(loss.item())
            GVecs.append(g_norm.item())

            print(g_norm.item(), loss.item())

            model.zero_grad()


        with open('./exps/LossVecs-{}.txt'.format(self.exp_name), 'a') as f:
            f.write(str(LossVecs) + '\n')

        with open('./exps/GVecs-{}.txt'.format(self.exp_name), 'a') as f:
            f.write(str(GVecs) + '\n')

        logits = model(**sample['net_input'], masked_tokens=masked_tokens)[0]
        targets = model.get_targets(sample, [logits])

        #import IPython
        #IPython.embed()
        if sample_size != 0:
            targets = targets[masked_tokens]

        loss = F.nll_loss(
            F.log_softmax(
                logits.view(-1, logits.size(-1)),
                dim=-1,
                dtype=torch.float32,
            ),
            targets.view(-1),
            reduction='sum',
            ignore_index=self.padding_idx,
        )


        token_loss = torch.tensor([0])
        logging_output = {
            'loss': utils.item(loss.data) if reduce else loss.data,
            'tokenloss': utils.item(token_loss.data) if reduce else token_loss.data,
            'ntokens': sample['ntokens'],
            'nsentences': sample['nsentences'],
            'sample_size': sample_size,
        }
        return loss, sample_size, logging_output

    @staticmethod
    def reduce_metrics(logging_outputs) -> None:
        """Aggregate logging outputs from data parallel training."""

        tokenloss_sum = sum(log.get('tokenloss', 0) for log in logging_outputs)
        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)


        metrics.log_scalar('tokenloss', tokenloss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=3)
        metrics.log_derived('ppl', lambda meters: round(2**meters['loss'].avg, 3))

    @staticmethod
    def logging_outputs_can_be_summed() -> bool:
        """
        Whether the logging outputs returned by `forward` can be summed
        across workers prior to calling `reduce_metrics`. Setting this
        to True will improves distributed training speed.
        """
        return True
